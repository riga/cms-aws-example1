{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook requires TF 2.3, check the tf version\n",
    "# on SWAN at CERN, this requires the bleeding edge stack (as of 17 Dec 2020)\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"TensorFlow {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "## Jet classification\n",
    "\n",
    "This example tries to distinguish between jets produced by a hadronically decaying top quark which hadronizes, to jets produced by a light flavour quark or a gluon.\n",
    "\n",
    "If the top quark has a very high transverse momentum, the decay products of the top (one b jet and two quark jets stemming from the decaying W boson), will be merged into one single large jet, which is referred to as a **top jet**. Potentially, this jet can exhibit three distinct, resolvable *sub jets*, whereas a light quark or gluon jet only appears as one single, large jet without any significant substructure.\n",
    "\n",
    "The different appearance of these jets can be used as a handle to discriminate between them. Being able to correctly identify top jets, and tell them apart from the overwhelming background of other light-flavored jets, is extremely important for many reasons.\n",
    "\n",
    "Since the top quark is so heavy, being the only fermion we know of with a mass on the order of the weak scale, several extensions of the Standard Model which attempt to solve the hierarchy problem predict large couplings of new, hitherto unobserved particles to top quarks. Weeding top quark jets out of the ocean of other jets is therefore crucial for many **New Physics** searches!\n",
    "\n",
    "\n",
    "## Input data\n",
    "\n",
    "The input data consists of 1 million jets, originating from either\n",
    "  - hadronically decaying top quarks (**signal**), or\n",
    "  - dijet QCD events (**background**),\n",
    " \n",
    "and clustered using the $k_{T}$ algorithm with $\\Delta R$ = 0.8.\n",
    "\n",
    "Data was generated using Phythia & Delphes, configured\n",
    "  - to collide protons at 14 TeV center-of-mass energy,\n",
    "  - to generate jets with a $p_{T}$ range of [550, 650] GeV (before hadronization), and\n",
    "  - **without** mixing in pileup events for simplicity.\n",
    "  \n",
    "The data is stored in NumPy arrays across several files, with 50k jets per file.\n",
    "\n",
    "- 20 training files (`\"train\"`)\n",
    "- 8 validation files (`\"valid\"`)\n",
    "- 8 testing files (`\"test\"`)\n",
    "\n",
    "\n",
    "## Features\n",
    "\n",
    "Per jet, the four-vectors of up to **200** of its *constituents* are given (800 features). Note that not all jets have that many constituents. To avoid the trouble of working with uneven (so-called *jagged*) arrays, these \"missing\" constituents vectors are filled with zeros.\n",
    "\n",
    "\n",
    "## Training targets\n",
    "\n",
    "Per jet, a flag is given that marks the true origin of the jet \n",
    "  - `1` for jets from top quark decays\n",
    "  - `0` for light jets from QCD events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load software and helpers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the content of two \"train\" files\n",
    "vectors, labels = load_data(\"train\", start_file=0, stop_file=2)\n",
    "vectors.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some flags to make four-vector element access more verbose\n",
    "E, PX, PY, PZ = range(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a histogram helper\n",
    "def plot_hist(arr, names=None, xlabel=None, ylabel=\"Entries\", filename=None, legend_loc=\"upper center\", **kwargs):\n",
    "    kwargs.setdefault(\"bins\", 20)\n",
    "    kwargs.setdefault(\"alpha\", 0.7)\n",
    "   \n",
    "    # consider multiple arrays and names given as a tuple\n",
    "    arrs = arr if isinstance(arr, tuple) else (arr,)\n",
    "    names = names or (len(arrs) * [\"\"])\n",
    "\n",
    "    # start plot\n",
    "    fig, ax = plt.subplots()\n",
    "    for arr, name in zip(arrs, names):\n",
    "        bin_edges = ax.hist(arr, label=name, **kwargs)[1]\n",
    "        kwargs[\"bins\"] = bin_edges\n",
    "    \n",
    "    # legend\n",
    "    if any(names):\n",
    "        legend = ax.legend(loc=legend_loc)\n",
    "        legend.get_frame().set_linewidth(0.0)\n",
    "    \n",
    "    # styles and custom adjustments\n",
    "    ax.tick_params(axis=\"both\", direction=\"in\", top=True, right=True)\n",
    "    if xlabel:\n",
    "        ax.set_xlabel(xlabel)\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel)\n",
    " \n",
    "    if filename:\n",
    "        fig.savefig(filename)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# distribution of truth labels\n",
    "plot_hist(labels, xlabel=\"Label distribution\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# number of constituents per jet\n",
    "# remember, missing constituents are filled with zeros, so we take the energy value as a marker\n",
    "n_c = np.count_nonzero(vectors[:, :, E], axis=1)\n",
    "plot_hist(n_c, xlabel=\"N constituents per jet\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# energy distribution of all constituents\n",
    "e_c = vectors[:, :, E].flatten()\n",
    "# store a mask to remove zeros\n",
    "non_zero = e_c != 0\n",
    "plot_hist(e_c[non_zero], log=True, xlabel=\"Constituents energy / GeV\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# px distribution of all constituents, zeros removed with the mask defined above\n",
    "px_c = vectors[:, :, PX].flatten()\n",
    "plot_hist(px_c[non_zero], log=True, xlabel=\"Constituents $p_x$ / GeV\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# pz distribution of all constituents\n",
    "pz_c = vectors[:, :, PZ].flatten()\n",
    "plot_hist(pz_c[non_zero], log=True, xlabel=\"Constituents $p_z$ / GeV\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we define a preprocessing function that (e.g.) takes the\n",
    "# constiuents and returns an other representation of them\n",
    "# in this case, we select only the first 120 constituents and\n",
    "# flatten the resulting array from (..., 120, 4) to (..., 480,)\n",
    "def preprocess_constituents(constituents):\n",
    "    return constituents[:, :120].reshape((-1, 480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also, for the training we need to convert the label to a \"one-hot\" representation\n",
    "# 0. -> [1., 0.]\n",
    "# 1. -> [0., 1.]\n",
    "def labels_to_onehot(labels):\n",
    "    labels = labels.astype(np.int32)\n",
    "    onehot = np.zeros((labels.shape[0], labels.max() + 1), dtype=np.float32)\n",
    "    onehot[np.arange(labels.shape[0]), labels] = 1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model generating function\n",
    "# - 4 hidden layers\n",
    "# - 128 units each\n",
    "# - tanh activation\n",
    "# - 2 output units with softmax activation\n",
    "# (applies exp() to outputs and normalizes sum of all outputs to 1)\n",
    "def create_model():\n",
    "    x = tf.keras.Input(shape=(480,))\n",
    "    a1 = tf.keras.layers.Dense(128, use_bias=True, activation=\"tanh\")(x)\n",
    "    a2 = tf.keras.layers.Dense(128, use_bias=True, activation=\"tanh\")(a1)\n",
    "    a3 = tf.keras.layers.Dense(128, use_bias=True, activation=\"tanh\")(a2)\n",
    "    a4 = tf.keras.layers.Dense(128, use_bias=True, activation=\"tanh\")(a3)\n",
    "    y = tf.keras.layers.Dense(2, use_bias=True, activation=\"softmax\")(a4)\n",
    "    return tf.keras.Model(inputs=x, outputs=y, name=\"toptagging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the actual model\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load more training, and also validation data\n",
    "vectors_train, labels_train = load_data(\"train\", stop_file=6)\n",
    "vectors_valid, labels_valid = load_data(\"valid\", stop_file=3)\n",
    "\n",
    "# run the preprocessing\n",
    "vectors_train = preprocess_constituents(vectors_train)\n",
    "vectors_valid = preprocess_constituents(vectors_valid)\n",
    "\n",
    "# create one-hot labels\n",
    "labels_train = labels_to_onehot(labels_train)\n",
    "labels_valid = labels_to_onehot(labels_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "# this means that the internal computational graph structure is built,\n",
    "# the loss function (the function that provides the feedback by comparing\n",
    "# expected and predicted result, more on that later), and metrics are\n",
    "# registered that are shown during the training\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the training for 5 epochs (running through all data 5 times)\n",
    "model.fit(\n",
    "    vectors_train,\n",
    "    labels_train,\n",
    "    batch_size=200,\n",
    "    epochs=2,\n",
    "    # callbacks=[llp.PlotLossesKerasTF(outputs=[llp.outputs.MatplotlibPlot(cell_size=(4, 2))])],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate all training and validation data again for ruther study\n",
    "predictions_train = model.predict(vectors_train)\n",
    "predictions_valid = model.predict(vectors_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the accuracy\n",
    "def calculate_accuracy(labels, predictions):\n",
    "    # while the labels (NumPy array) are one-hot encoded,\n",
    "    # each prediction (TF tensor) consists of two numbers whose sum is 1,\n",
    "    # so we interpret the prediction to be the signal when the second value (index 1) is > 0.5\n",
    "    # hence, we can use argmax\n",
    "    predicted_top = np.argmax(predictions, axis=-1) == 1\n",
    "    labels_top = labels[:, 1] == 1\n",
    "    return (predicted_top == labels_top).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = calculate_accuracy(labels_train, predictions_train)\n",
    "acc_valid = calculate_accuracy(labels_valid, predictions_valid)\n",
    "\n",
    "print(f\"train accuracy: {acc_train:.4f}\")\n",
    "print(f\"valid accuracy: {acc_valid:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(\n",
    "    (predictions_valid[labels_valid == 0], predictions_valid[labels_valid == 1]),\n",
    "    names=(\"Light jets\", \"Top jets\"),\n",
    "    xlabel=\"Output distribution\",\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
